# sample_naive
arch:
  CRITIC_HAS_ACT_IND: False
  ACTIVATION:
    actor: Sin
    critic: Sin
  APPEND_DIM: 0
  LATENT_DIM: 0
  DIM_LIST:
    actor:  #! neurons for hidden layers
    - 256
    - 256
    - 256
    critic:  #! neurons for hidden layers
    - 128
    - 128
    - 128
  ACTION_RANGE:
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
  ACTION_DIM: 12  # the number of joints
  OBS_DIM: 32

environment:
  SEED: 0
  NUM_AGENTS: 1
  TIMEOUT: 300
  END_CRITERION: failure

solver:
  USE_WANDB: False
  PROJECT_NAME: spirit-rl-pybullet
  NAME: debug
  OUT_FOLDER: train_result/test_spirit_refactor/test_l1
  CHECK_OPT_FREQ: 10
  SAVE_TOP_K: 20
  SAVE_METRIC: safety
  # train
  NUM_CPUS: 1
  MAX_STEPS: 6_000_000
  MEMORY_CAPACITY: 1_000_000
  MIN_STEPS_B4_OPT: 100_000
  OPTIMIZE_FREQ: 10_000
  UPDATE_PER_OPT: 1_000
  # eval
  NUM_EVAL_TRAJ: 20
  EVAL_TIMEOUT: 300
  NUM_ENVS: 1
  WARMUP_ACTION_RANGE:
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
  OBS_DIM: 32
  ROLLOUT_END_CRITERION: reach-avoid
  VENV_DEVICE: cpu

agent:
  DYN: SpiritPybullet
  FOOTPRINT: None
  VERBOSE: False
  GUI: False
  GUI_IMAGINARY: False
  DT: 0.02
  APPLY_FORCE: False # warning, currently this will also affect adversarial force
  REPLACE_ADV_WITH_DR: False # If True, this will replace the adversarial force with DR force
  FORCE: 0
  FORCE_SCALE: 1.0
  FORCE_RESET_TIME: 50
  FORCE_INFO:
  LINK_NAME: 
  ROTATE_RESET: True
  HEIGHT_RESET: drop
  FORCE_RANDOM: True
  TERRAIN: normal
  TERRAIN_HEIGHT: 0.1
  TERRAIN_GRIDSIZE: 0.2
  TERRAIN_FRICTION: 1.0
  ENVTYPE: normal
  ACTION_RANGE:
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
  OBS_DIM: 32
  NUM_SEGMENT: 1 # not use, dummy info for Agent class
  AGENT_ID: ego
  RESET_CRITERION: failure
    
update:
  MAX_MODEL: 20
  ALPHA: 0.1
  LEARN_ALPHA: True
  BATCH_SIZE: 256
  DEVICE: cpu
  OPT_TYPE: AdamW
  GAMMA: 0.9
  GAMMA_DECAY: 0.1
  GAMMA_END: 0.999
  GAMMA_PERIOD: 1_000_000
  GAMMA_SCHEDULE: True
  LATENT_DIM: 0
  LR_A: 0.0001
  LR_C: 0.0001
  LR_Al: 0.000025
  LR_A_END: 0.0001
  LR_C_END: 0.0001
  LR_Al_END: 0.00005
  LR_A_PERIOD: 50000
  LR_C_PERIOD: 50000
  LR_Al_PERIOD: 100000
  LR_A_DECAY: 0.9
  LR_C_DECAY: 0.9
  LR_Al_DECAY: 0.9
  LR_A_SCHEDULE: False
  LR_C_SCHEDULE: False
  LR_Al_SCHEDULE: False
  MODE: reach-avoid
  TAU: 0.01
  TERMINAL_TYPE: max
  EVAL: False
  UPDATE_PERIOD: 2 # of the actor
  ACTOR_TYPE: min

eval:
  MODEL_TYPE: manual # highest, safest, manual
  STEP: 4_000_000 # the step to use if "manual" is chosen for MODEL_TYPE
  EVAL_TIMEOUT: 300 # how long do we evaluate in real rollout env
  IMAGINARY_HORIZON: 300 # the horizon of the imaginary env