environment:
  seed: 0
  timeout: 300
  end_criterion: failure

agent:
  agent_id: ego
  dyn: SpiritPybullet
  footprint: none
  verbose: false
  gui: false
  gui_imaginary: false
  dt: 0.02
  apply_force: false # warning, currently this will also affect adversarial force
  replace_adv_with_dr: false # if true, this will replace the adversarial force with dr force
  force: 0
  force_scale: 1.0
  force_reset_time: 50
  force_info:
  link_name: 
  rotate_reset: true
  height_reset: drop
  force_random: true
  terrain: normal
  terrain_height: 0.1
  terrain_gridsize: 0.2
  terrain_friction: 1.0
  envtype: normal
  action_range: &ctrl_range
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
    - [-0.5, 0.5]
  obs_dim: 32
  reset_criterion: failure

solver:
  device: cpu
  rollout_env_device: cpu
  num_envs: 1
  num_actors: 1
  num_critics: 1
  # == hyperparameters of learning ==
  max_steps: 6_000_000
  opt_period: 10_000
  num_updates_per_opt: 1_000
  eval_period: 100_000
  min_steps_b4_opt: 100_000
  warmup_steps: 100_000
  batch_size: 256
  warmup_action_range: *ctrl_range
  memory_capacity: 1_000_000
  # == logging ==
  use_wandb: false
  project_name: spirit-rl-pybullet
  name: debug
  max_model: 20
  save_top_k: 20
  out_folder: train_result/test_spirit_refactor/test_l1
  # == hyperparameters of actors and critics ==
  critic_0:
    eval: false
    net_name: central
    lr: 0.0001
    lr_schedule: false
    lr_end: 0.0001
    lr_period: 50000
    lr_decay: 0.9
    gamma: 0.9
    gamma_decay: 0.1
    gamma_end: 0.999
    gamma_period: 1_000_000
    gamma_schedule: true
    tau: 0.01
    terminal_type: all  # use min{l_x, g_x} for terminal states/obsrvs.
    opt_type: AdamW
    update_target_period: 2
    mode: reach-avoid
  actor_0:
    eval: false
    net_name: ctrl
    actor_type: max
    learn_alpha: true
    lr: 0.0001
    lr_al: 0.000025
    lr_schedule: false
    lr_al_schedule: false
    alpha: 0.1
    min_alpha: 0.01
    opt_type: AdamW
    update_period: 2
    lr_end: 0.0001
    lr_al_end: 0.00005
    lr_period: 50000
    lr_al_period: 100000
    lr_decay: 0.9    
    lr_al_decay: 0.9
  eval:   
    b4_learn: false
    metric: safety
    to_max: true
    num_trajectories: 20
    timeout: 300
    end_criterion: reach-avoid
  obs_dim: 32
  rollout_end_criterion: reach-avoid

arch:
  actor_0:
    mlp_dim:
      - 256
      - 256
      - 256
    activation: Sin
    append_dim: 0
    latent_dim: 0
    obsrv_dim: 32
    action_dim: 12
    action_range: *ctrl_range
  critic_0:
    mlp_dim:
      - 128
      - 128
      - 128
    activation: Sin
    append_dim: 0
    latent_dim: 0
    obsrv_dim: 32
    action_dim: 12

eval:
  model_type: manual # highest, safest, manual
  step: 4_000_000 # the step to use if "manual" is chosen for model_type
  eval_timeout: 300 # how long do we evaluate in real rollout env
  imaginary_horizon: 300 # the horizon of the imaginary env